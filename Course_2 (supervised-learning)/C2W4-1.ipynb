{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d40100f3",
   "metadata": {},
   "source": [
    "### Бэггинг и случайный лес"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "518013c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6d13782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble, model_selection, datasets, tree \n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a5940e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем датасет digits с помощью функции load_digits из sklearn.datasets \n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f6d3c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подготавливаем матрицу признаков X и ответов на обучающей выборке y\n",
    "\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa2f10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пишем функцию записи ответов в текстовый файл\n",
    "\n",
    "def write_answer(answer, n):\n",
    "  with open('C2W4_answer_{}.txt'.format(n), 'w') as f:\n",
    "    f.write(str(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a68805",
   "metadata": {},
   "source": [
    "### Задание 1. \n",
    "Создайте DecisionTreeClassifier с настройками по умолчанию и измерьте качество его работы с помощью cross_val_score. \n",
    "Эта величина и будет ответом в пункте 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e031a364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc = tree.DecisionTreeClassifier()\n",
    "dtc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9572ce58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8235940409683427\n"
     ]
    }
   ],
   "source": [
    "res1 = cross_val_score(dtc, X, y, cv=10).mean()\n",
    "print(res1)\n",
    "write_answer(res1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7461b",
   "metadata": {},
   "source": [
    "### Задание 2. \n",
    "Воспользуйтесь BaggingClassifier из sklearn.ensemble, чтобы обучить бэггинг над DecisionTreeClassifier. Используйте в BaggingClassifier параметры по умолчанию, задав только количество деревьев равным 100. \n",
    "Качество классификации новой модели - ответ в пункте 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b7f43d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(n_estimators=100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bc1 = ensemble.BaggingClassifier(n_estimators=100)\n",
    "bc1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fd0d3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9248261949099937\n"
     ]
    }
   ],
   "source": [
    "res2 = cross_val_score(bc1, X, y, cv=10).mean()\n",
    "print(res2)\n",
    "write_answer(res2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5fbec",
   "metadata": {},
   "source": [
    "### Задание 3. \n",
    "Теперь изучите параметры BaggingClassifier и выберите их такими, чтобы каждый базовый алгоритм обучался не на всех d признаках, а на sqrt{d} случайных признаков. \n",
    "Качество работы получившегося классификатора - ответ в пункте 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f36b1c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(max_features=8, n_estimators=100)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt_d = int(np.sqrt(X.shape[1]))\n",
    "bc2 = ensemble.BaggingClassifier(n_estimators=100, max_features=sqrt_d)\n",
    "bc2.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0bc0c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9337895716945995\n"
     ]
    }
   ],
   "source": [
    "res3 = cross_val_score(bc2, X, y, cv=10).mean()\n",
    "print(res3)\n",
    "write_answer(res3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20501db",
   "metadata": {},
   "source": [
    "### Задание 4.\n",
    "Наконец, давайте попробуем выбирать случайные признаки не один раз на все дерево, а при построении каждой вершины дерева. Сделать это несложно: нужно убрать выбор случайного подмножества признаков в BaggingClassifier и добавить его в DecisionTreeClassifier. Попробуйте выбирать опять же sqrt{d} признаков. \n",
    "Качество полученного классификатора на контрольной выборке и будет ответом в пункте 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f6c1782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(max_features=8),\n",
       "                  n_estimators=100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = tree.DecisionTreeClassifier(max_features = sqrt_d)\n",
    "bc3 = ensemble.BaggingClassifier(base_estimator=tree, n_estimators=100)\n",
    "bc3.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb2104d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9432309124767224\n"
     ]
    }
   ],
   "source": [
    "res4 = cross_val_score(bc3, X, y, cv=10).mean()\n",
    "print(res4)\n",
    "write_answer(res4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c467568c",
   "metadata": {},
   "source": [
    "### Задание 5. \n",
    "Полученный в пункте 4 классификатор - бэггинг на рандомизированных деревьях (в которых при построении каждой вершины выбирается случайное подмножество признаков и разбиение ищется только по ним). Это в точности соответствует алгоритму Random Forest, поэтому почему бы не сравнить качество работы классификатора с RandomForestClassifier из sklearn.ensemble. Сделайте это, а затем изучите, как качество классификации на данном датасете зависит от количества деревьев, количества признаков, выбираемых при построении каждой вершины дерева, а также ограничений на глубину дерева. Для наглядности лучше построить графики зависимости качества от значений параметров, но для сдачи задания это делать не обязательно. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "123d08de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "rfc.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2973dea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9454562383612662\n"
     ]
    }
   ],
   "source": [
    "res = cross_val_score(rfc, X, y, cv=10).mean()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a0d06",
   "metadata": {},
   "source": [
    "### Выводы:\n",
    "- 2) При очень маленьком числе деревьев (5, 10, 15), случайный лес работает хуже, чем при большем числе деревьев.\n",
    "- 3) С ростом количества деревьев в случайном лесе, в какой-то момент деревьев становится достаточно для высокого качества классификации, а затем качество существенно не меняется.\n",
    "- 4) При большом количестве признаков (для данного датасета - 40, 50) качество классификации становится хуже, чем при малом количестве признаков (5, 10). Это связано с тем, что чем меньше признаков выбирается в каждом узле, тем более различными получаются деревья (ведь деревья сильно неустойчивы к изменениям в обучающей выборке), и тем лучше работает их композиция.\n",
    "- 7) При небольшой максимальной глубине деревьев (5-6) качество работы случайного леса заметно хуже, чем без ограничений, т.к. деревья получаются недообученными. С ростом глубины качество сначала улучшается, а затем не меняется существенно, т.к. из-за усреднения прогнозов и различий деревьев их переобученность в бэггинге не сказывается на итоговом качестве (все деревья преобучены по-разному, и при усреднении они компенсируют переобученность друг-друга)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b21f1734",
   "metadata": {},
   "outputs": [],
   "source": [
    "res5 = '2 3 4 7'\n",
    "write_answer(res5, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
